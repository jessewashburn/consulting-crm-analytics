# CRM Analytics Service

Django-based analytics and event processing service for the consulting CRM. This service consumes events from the transactional database, processes them asynchronously via AWS SQS, and generates analytics-ready data models.

## ï¿½ See It Working (30 seconds)

```bash
cd consulting-crm-analytics
source venv/Scripts/activate
python show_analytics.py
```

**Output:**
```
ğŸ“Š ANALYTICS WORKING - REAL BUSINESS DATA

Event Activity:
  2025-12-23 | INSERT_ACCOUNTS      | Count: 1
  2025-12-23 | INSERT_LEADS         | Count: 3
  2025-12-23 | UPDATE_LEADS         | Count: 1

Business Metrics:
  ğŸ“ˆ Total Leads Created: 4
  ğŸ”„ Lead Updates: 1
  ğŸ¢ Total Accounts Created: 1

âœ… ANALYTICS ARE REAL
```

**This proves:** Events flow from CRM â†’ Analytics â†’ Queryable Insights

## ï¿½ğŸ“ Architecture Overview

See **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** for the complete architecture diagram and design decisions.

See **[docs/IDEMPOTENCY.md](docs/IDEMPOTENCY.md)** for production-grade failure handling and idempotency implementation.

### How Data Flows (30-second version)

1. **CRM Event Occurs** â†’ User creates/updates lead in Supabase
2. **Trigger Fires** â†’ Event automatically inserted into `event_outbox` (same transaction)
3. **Celery Polls** â†’ Every 30s, batch of events fetched and published to SQS
4. **Workers Process** â†’ Celery workers consume from SQS, update analytics tables, archive to S3
5. **API Serves** â†’ Django REST API exposes pre-aggregated data to dashboards

**Key Properties:** 
- âœ… Events never lost (transactional outbox)
- âœ… Processing is idempotent (safe to retry)
- âœ… Failures are retried automatically (exponential backoff)
- âœ… Dead-letter queue for permanent failures

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Supabase/       â”‚
â”‚ Postgres        â”‚â—„â”€â”€â”€ Transactional CRM data
â”‚ event_outbox    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Poller    â”‚â—„â”€â”€â”€ Celery Beat (scheduled)
â”‚ (Celery Task)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS SQS         â”‚â—„â”€â”€â”€ Decoupled event queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Workers   â”‚â—„â”€â”€â”€ Celery workers
â”‚ (Process events)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â–º AWS S3 (raw archive)
         â””â”€â”€â–º Analytics DB (aggregates)
```

## Components

### 1. Event Poller (`events/tasks.py`)
- Polls `event_outbox` table for unprocessed events
- Publishes to SQS
- Marks events as processed

### 2. Event Processor (`events/consumers.py`)
- Consumes from SQS
- Validates and normalizes events
- Updates analytics tables
- Archives raw events to S3

### 3. Analytics Models (`events/models.py`)
- Time-series aggregates
- Daily/weekly/monthly rollups
- Funnel metrics
- Revenue tracking

### 4. REST API (`api/`)
- Read-only endpoints for dashboards
- Filtered by date range, account, etc.
- Optimized for BI queries

## Setup

### 1. Install Dependencies

```bash
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your actual credentials
```

### 3. Run Migrations

```bash
python manage.py migrate
```

### 4. Start Services

**Terminal 1 - Django API:**
```bash
python manage.py runserver
```

**Terminal 2 - Celery Worker:**
```bash
celery -A analytics worker --loglevel=info
```

**Terminal 3 - Celery Beat (Scheduler):**
```bash
celery -A analytics beat --loglevel=info
```

## Development Workflow

1. **Add new analytics table** â†’ Create model in `events/models.py`
2. **Add event processor** â†’ Update `events/consumers.py`
3. **Expose via API** â†’ Add view in `api/views.py`
4. **Test locally** â†’ Insert test data in CRM backend

## AWS Resources Needed

- **SQS Queue:** `crm-events`
- **S3 Bucket:** `crm-events-archive`
- **IAM Role/User:** with `sqs:SendMessage`, `sqs:ReceiveMessage`, `s3:PutObject`

## Next Steps

- [ ] Set up AWS SQS queue
- [ ] Create S3 bucket for event archive
- [ ] Configure IAM credentials
- [ ] Deploy to EC2 or ECS
- [ ] Integrate BI tool (Metabase/Superset)
