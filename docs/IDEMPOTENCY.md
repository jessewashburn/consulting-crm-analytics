# Idempotency & Failure Handling

## Overview

This system implements **production-grade reliability** through:

1. **Idempotent processing** - Events can be safely replayed without double-counting
2. **Automatic retries** - Failed events retry with exponential backoff
3. **Dead-letter queue** - Permanently failed events logged for manual investigation
4. **Deterministic event IDs** - Each event has a unique, stable identifier

## How It Works

### 1. Idempotency Keys

Each event gets a unique `event_id` (UUID) when created:

```sql
CREATE TABLE event_outbox (
    id uuid PRIMARY KEY,                    -- Internal DB ID
    event_id text UNIQUE NOT NULL,          -- Idempotency key ðŸ”‘
    event_type text,
    aggregate_type text,
    aggregate_id uuid,
    payload jsonb,
    ...
);
```

Before processing, we check if this `event_id` was already handled:

```python
if ProcessedEvent.objects.filter(event_id=event_id).exists():
    logger.warning(f"Event {event_id} already processed, skipping")
    return  # Safe to skip!
```

**Result:** If a message is delivered twice (SQS retry, worker restart, etc.), it's safely ignored.

### 2. Retry Logic with Exponential Backoff

Failed events retry automatically:

```python
MAX_RETRIES = 3
RETRY_BACKOFF_SECONDS = [60, 300, 900]  # 1min, 5min, 15min

# On failure
if retry_count < MAX_RETRIES:
    raise self.retry(countdown=RETRY_BACKOFF_SECONDS[retry_count])
```

**Retry schedule:**
- Attempt 1: Immediate
- Attempt 2: After 1 minute
- Attempt 3: After 5 minutes  
- Attempt 4: After 15 minutes
- After 4 failures â†’ Dead letter queue

### 3. Dead Letter Queue (failed_events table)

After max retries, events move to `failed_events`:

```sql
CREATE TABLE failed_events (
    id uuid PRIMARY KEY,
    event_id text UNIQUE,
    event_type text,
    payload jsonb,
    error_message text,        -- What went wrong
    error_trace text,           -- Full stack trace
    retry_count int,            -- How many times we tried
    first_failed_at timestamptz,
    last_failed_at timestamptz,
    resolved_at timestamptz,    -- NULL = still broken
    resolved_by text            -- Who fixed it
);
```

**Benefits:**
- Nothing lost - all failures recorded
- Full context for debugging (payload + error + trace)
- Manual replay capability
- Alert on new failures

### 4. Monitoring Points

Track system health:

```python
# Count unprocessed events
SELECT COUNT(*) FROM event_outbox WHERE processed_at IS NULL;

# Count active failures
SELECT COUNT(*) FROM failed_events WHERE resolved_at IS NULL;

# Recent failures
SELECT event_type, error_message, first_failed_at 
FROM failed_events 
WHERE resolved_at IS NULL
ORDER BY first_failed_at DESC;
```

## Testing Idempotency

### Test 1: Duplicate Event Delivery

```python
# Send same event twice
event_id = "test-event-123"
process_event(event_id=event_id, event_type="INSERT_LEADS", ...)
process_event(event_id=event_id, event_type="INSERT_LEADS", ...)  # Duplicate

# Check: Only 1 record in processed_events, no double-counting
```

### Test 2: Retry After Failure

```python
# Simulate transient failure (DB timeout, API rate limit, etc.)
def flaky_process():
    if random.random() < 0.7:  # 70% failure rate
        raise Exception("Transient error")
    return "success"

# System will automatically retry up to 3 times
```

### Test 3: Permanent Failure

```python
# Simulate permanent failure (bad data, invalid foreign key, etc.)
payload = {"account_id": "invalid-uuid"}  # Will always fail

# After 4 attempts:
# 1. Event moved to failed_events table
# 2. Alert sent (email/Slack)
# 3. Original event_outbox marked as processed (won't retry forever)
```

## Manual Operations

### View Failed Events

```sql
SELECT 
    event_id,
    event_type,
    error_message,
    retry_count,
    first_failed_at,
    payload::jsonb ->> 'company_name' as company
FROM failed_events
WHERE resolved_at IS NULL
ORDER BY first_failed_at DESC
LIMIT 10;
```

### Replay a Failed Event

After fixing the root cause (bad data, missing FK, etc.):

```python
from events.tasks import replay_failed_event

# Get the failed event ID from the database
failed_event_id = "uuid-from-query-above"

# Replay it
replay_failed_event.delay(failed_event_id)
```

The replay task will:
1. Remove from `processed_events` (allow reprocessing)
2. Retry with `retry_count=0`
3. Mark as `resolved_at` if successful

### Bulk Replay

```python
# Replay all failures of a certain type
from events.models import FailedEvent

for fe in FailedEvent.objects.filter(
    event_type="INSERT_PROJECTS",
    resolved_at__isnull=True
):
    replay_failed_event.delay(str(fe.id))
```

## Architecture Benefits

### Why This Approach?

| Problem | Solution | Benefit |
|---------|----------|---------|
| Network failure during processing | Idempotency check via `event_id` | Safe to retry, no duplicates |
| Transient errors (DB timeout, rate limit) | Exponential backoff retries | Auto-recovers from blips |
| Permanent errors (bad data) | Dead-letter queue | Don't lose events, can investigate |
| Worker crashes mid-processing | Transaction atomicity | All-or-nothing updates |
| Need to replay events | Immutable event log in S3 | Full audit trail |

### Production Readiness Signals

âœ… **Idempotency** - "What if this message is delivered twice?"  
âœ… **Retries** - "What if the database is temporarily unavailable?"  
âœ… **Dead-letter queue** - "What if this event can never succeed?"  
âœ… **Observability** - "How do I know if the system is healthy?"  
âœ… **Manual intervention** - "How do I fix a stuck event?"

These are the questions **senior engineers ask**. This system answers them all.

## Cost of Failure Handling

**Minimal:**
- `processed_events` table: ~100 bytes per event
- `failed_events` table: Only stores actual failures (~0.1% typically)
- Retry overhead: Negligible with exponential backoff

**ROI:**
- Zero data loss
- Automatic recovery from transient issues
- Clear debugging path for permanent issues
- Sleep well at night ðŸ˜´

## Comparison to Naive Approach

### Without Idempotency

```python
# Naive approach
def process_event(event):
    lead_count += 1  # ðŸ’¥ Double counted if event delivered twice!
```

### With Idempotency

```python
def process_event(event_id, event):
    if already_processed(event_id):
        return  # âœ… Safe!
    
    lead_count += 1
    mark_processed(event_id)
```

**Real-world scenario:**
- SQS delivers message
- Worker processes it, crashes before ACK
- SQS re-delivers (assumes failure)
- Without idempotency: Double counted âŒ
- With idempotency: Safely ignored âœ…

## Alerting (TODO)

Set up alerts for:

```python
# Alert if unprocessed events > 1000
SELECT COUNT(*) FROM event_outbox WHERE processed_at IS NULL;

# Alert on new failures
SELECT COUNT(*) FROM failed_events 
WHERE first_failed_at > NOW() - INTERVAL '1 hour';

# Alert if oldest unprocessed event > 5 minutes
SELECT MIN(created_at) FROM event_outbox WHERE processed_at IS NULL;
```

**Delivery options:**
- Email via AWS SES
- Slack webhook
- PagerDuty
- CloudWatch Alarms

## Further Reading

- [Transactional Outbox Pattern](https://microservices.io/patterns/data/transactional-outbox.html)
- [Idempotency in Distributed Systems](https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/)
- [AWS SQS Best Practices](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html)
